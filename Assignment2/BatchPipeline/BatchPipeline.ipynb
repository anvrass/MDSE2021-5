{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6082e75e-8022-421f-b8da-fe8765c5b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order_ID: long (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity_Ordered: integer (nullable = true)\n",
      " |-- Price_Each: double (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Purchase_Address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+-------------------+\n",
      "|Order_ID|             Product|Quantity_Ordered|Price_Each|    Order_Date|    Purchase_Address|          timestamp|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+-------------------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|12/30/19 00:01|136 Church St, Ne...|2019-12-30 00:01:00|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|12/29/19 07:03|562 2nd St, New Y...|2019-12-29 07:03:00|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|12/12/19 18:21|277 Main St, New ...|2019-12-12 18:21:00|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|12/22/19 15:13|410 6th St, San F...|2019-12-22 15:13:00|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|12/18/19 12:38|43 Hill St, Atlan...|2019-12-18 12:38:00|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|12/31/19 22:58|200 Jefferson St,...|2019-12-31 22:58:00|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|12/16/19 15:10|928 12th St, Port...|2019-12-16 15:10:00|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|12/13/19 09:29|813 Hickory St, D...|2019-12-13 09:29:00|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|12/15/19 23:26|718 Wilson St, Da...|2019-12-15 23:26:00|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|12/28/19 11:51|77 7th St, Dallas...|2019-12-28 11:51:00|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, DateType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"GCSExample\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Create data frame, we use GS bucket as data sink\n",
    "\n",
    "# Google Storage File Path\n",
    "gsc_file_path = 'gs://dejads_input_assignment2/' \n",
    "\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"Order_ID\", LongType(), True),\n",
    "     StructField(\"Product\", StringType(), True),\n",
    "     StructField(\"Quantity_Ordered\", IntegerType(), True),\n",
    "     StructField(\"Price_Each\", DoubleType(), True),\n",
    "     StructField(\"Order_Date\", StringType(), True),\n",
    "     StructField(\"Purchase_Address\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "#Read the data of all months\n",
    "sales_df = spark.read.format(\"csv\").schema(dataSchema).option(\"header\", \"true\") \\\n",
    "       .load(gsc_file_path+'*.csv')\n",
    "\n",
    "#Drop null rows\n",
    "sales_df = sales_df.na.drop(\"any\")\n",
    "\n",
    "#Convert \"Order Date\" to data type \"timestamp\"\n",
    "sales_df = sales_df.withColumn(\"timestamp\",to_timestamp(col(\"Order_Date\"),'MM/dd/yy HH:mm')) \\\n",
    "\n",
    "sales_df.printSchema()\n",
    "sales_df.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1dd7d-9165-4c4e-9a03-c726dd250bbe",
   "metadata": {},
   "source": [
    "# Return the names of ordered products from the largest order of each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58da82f5-0004-44c8-a2b8-eb1503c77b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------+------------+\n",
      "|             Product|Price_Each|    Order_Date|Order_Amount|\n",
      "+--------------------+----------+--------------+------------+\n",
      "|     ThinkPad Laptop|    999.99|01/31/19 17:47|     1999.98|\n",
      "|  Macbook Pro Laptop|    1700.0|02/26/19 12:38|      2400.0|\n",
      "|              iPhone|     700.0|02/26/19 12:38|      2400.0|\n",
      "|Apple Airpods Hea...|     150.0|03/22/19 22:44|      1850.0|\n",
      "|  Macbook Pro Laptop|    1700.0|03/27/19 17:07|      1850.0|\n",
      "|Apple Airpods Hea...|     150.0|03/27/19 17:07|      1850.0|\n",
      "|  Macbook Pro Laptop|    1700.0|03/22/19 22:44|      1850.0|\n",
      "|  Macbook Pro Laptop|    1700.0|04/27/19 21:01|     3779.99|\n",
      "|34in Ultrawide Mo...|    379.99|04/27/19 21:01|     3779.99|\n",
      "|  Macbook Pro Laptop|    1700.0|05/25/19 13:19|      3400.0|\n",
      "|  Macbook Pro Laptop|    1700.0|05/13/19 13:40|      3400.0|\n",
      "|  Macbook Pro Laptop|    1700.0|06/08/19 09:00|      3400.0|\n",
      "|    Wired Headphones|     11.99|07/29/19 20:00|     2323.98|\n",
      "|  Macbook Pro Laptop|    1700.0|07/29/19 20:00|     2323.98|\n",
      "|        Google Phone|     600.0|07/29/19 20:00|     2323.98|\n",
      "|  Macbook Pro Laptop|    1700.0|08/26/19 12:57|      3400.0|\n",
      "|  Macbook Pro Laptop|    1700.0|09/26/19 11:58|      3400.0|\n",
      "|     ThinkPad Laptop|    999.99|10/04/19 11:29|     2699.99|\n",
      "|  Macbook Pro Laptop|    1700.0|10/04/19 11:29|     2699.99|\n",
      "|        Google Phone|     600.0|11/29/19 15:12|      2300.0|\n",
      "|  Macbook Pro Laptop|    1700.0|11/15/19 20:08|      2300.0|\n",
      "|  Macbook Pro Laptop|    1700.0|11/29/19 15:12|      2300.0|\n",
      "|        Google Phone|     600.0|11/15/19 20:08|      2300.0|\n",
      "|  Macbook Pro Laptop|    1700.0|12/01/19 10:31|     2699.99|\n",
      "|     ThinkPad Laptop|    999.99|12/01/19 10:31|     2699.99|\n",
      "+--------------------+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Find total price for each line\n",
    "sales_df = sales_df.withColumn(\"totalprice\",col(\"Price_Each\") * col(\"Quantity_Ordered\"))\n",
    "\n",
    "#Add total Order Amount to the table\n",
    "order_amounts = sales_df.groupBy(\"Order_ID\").agg(expr(\"sum(totalprice)\").alias(\"Order_Amount\"))\n",
    "joined_data = sales_df.join(order_amounts, [\"Order_ID\"], \"left\" )\n",
    "\n",
    "#Rank the orders based on the total amount for given month\n",
    "\n",
    "joined_data = joined_data.withColumn(\"month\", month(\"timestamp\"))\n",
    "\n",
    "window = Window.partitionBy(\"month\").orderBy(col(\"Order_Amount\").desc())\n",
    "joined_data = joined_data.withColumn(\"rank\", rank().over(window))\n",
    "\n",
    "joined_data = joined_data.na.drop(\"any\")\n",
    "\n",
    "#Print the products of the largest order for given day\n",
    "results = joined_data.where((col(\"rank\") == 1)).select(\"Product\",\"Price_Each\",\"Order_Date\",\"Order_Amount\").distinct().orderBy(col(\"month\"))\n",
    "results. show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d61f387-ee4c-4039-9953-3fb6edaaf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results to google cloud storage bucket\n",
    "\n",
    "results.write.format(\"csv\").save(\"gs://dejads_output_assignment2/batch_result3.csv\") # use correct bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230e97c1-1354-4dc5-888e-eb91e0db41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"dejads_output_assignment2\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "# Saving the data to BigQuery\n",
    "results.write.format('bigquery') \\\n",
    "  .option('table', 'de2021-325520.a2_dataset.products') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76da773a-f6be-4ed2-9b50-4c094025f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
