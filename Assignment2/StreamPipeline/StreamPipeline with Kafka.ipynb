{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d7d109de-bf68-40dd-8cb7-c86e14528226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- Subreddit: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- num_comments: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- timestamp_in_ms: string (nullable = true)\n",
      " |-- num_comments_int: integer (nullable = true)\n",
      " |-- score_int: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit, from_csv\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, IntegerType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Assignment2_stream\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"title\", StringType(), True),\n",
    "     StructField(\"score\", StringType(), True),\n",
    "     StructField(\"id\", StringType(), True),\n",
    "     StructField(\"Subreddit\", StringType(), True),\n",
    "     StructField(\"url\", StringType(), True),\n",
    "     StructField(\"num_comments\", StringType(), True),\n",
    "     StructField(\"body\", StringType(), True),\n",
    "     StructField(\"timestamp_in_ms\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "kafkaStream = spark \\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"subscribe\", \"reddit\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "#Create structured data frame from Kafka stream\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "df1 = df.select(from_csv(df.value, dataSchema.simpleString()))\n",
    "sdf = df1.select(col(\"from_csv(value).*\"))\n",
    "\n",
    "#type casting\n",
    "sdf = sdf.withColumn(\"num_comments_int\", col(\"num_comments\").cast(\"int\"))\n",
    "sdf = sdf.withColumn(\"score_int\", col(\"score\").cast(\"int\"))\n",
    "sdf.printSchema()\n",
    "\n",
    "withEventTimedf = sdf.selectExpr(\n",
    "    \"*\",\n",
    "    \"cast(timestamp_in_ms/1000.0 as timestamp) as event_time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "31c25582-4044-46cd-8b50-46fbb8039ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate correlation score between number of comments and score of a topic.\n",
    "corr_score = withEventTimedf.select(corr(\"num_comments_int\", \"score_int\").alias(\"Score_Comment_Corr\"))\n",
    "corr_score = corr_score.withColumn(\"Group\", lit(\"All Data Score\")) \n",
    "\n",
    "resultsdf = corr_score\n",
    "resultsdf = resultsdf.withColumn(\"value\", col(\"Score_Comment_Corr\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "89903146-a7aa-4d05-b58b-48868a4faa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf = resultsdf.select(col(\"Group\").alias(\"key\"), col(\"value\").alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "529c7995-4e62-4b98-b727-3bba0b81e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "query = resultdf \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\") \\\n",
    "    .option(\"topic\", \"corr_score\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7dab059c-9f99-49a2-8e4b-3d0b021c1cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+--------------+------------------+\n",
      "|           key|             value|\n",
      "+--------------+------------------+\n",
      "|All Data Score|0.7569652509225021|\n",
      "+--------------+------------------+\n",
      "\n",
      "+--------------+------------------+\n",
      "|           key|             value|\n",
      "+--------------+------------------+\n",
      "|All Data Score|0.7569652509225021|\n",
      "+--------------+------------------+\n",
      "\n",
      "+--------------+------------------+\n",
      "|           key|             value|\n",
      "+--------------+------------------+\n",
      "|All Data Score|0.7569652509225021|\n",
      "+--------------+------------------+\n",
      "\n",
      "+--------------+------------------+\n",
      "|           key|             value|\n",
      "+--------------+------------------+\n",
      "|All Data Score|0.7569652509225021|\n",
      "+--------------+------------------+\n",
      "\n",
      "+--------------+------------------+\n",
      "|           key|             value|\n",
      "+--------------+------------------+\n",
      "|All Data Score|0.7569652509225021|\n",
      "+--------------+------------------+\n",
      "\n",
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "query2 = resultdf \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"avg_score_window\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    for x in range(100):\n",
    "        spark.sql(\"SELECT * FROM avg_score_window\").show()\n",
    "        sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "52f6d09e-d0d4-47d6-977f-0e67c34ed5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "228f1413-d203-498f-aedf-6108e4fe57ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+\n",
      "| Score_Comment_Corr|     Group|              value|\n",
      "+-------------------+----------+-------------------+\n",
      "|0.44981273791379345|High Score|0.44981273791379345|\n",
      "|-0.7249747545295777| Low Score|-0.7249747545295777|\n",
      "+-------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The following operations are not used because of the limited usage of aggregate functions with streaming \n",
    "#data we got error.However, \n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "ordered_desc_df = withEventTimedf.orderBy(col(\"score_int\").desc())\n",
    "ordered_asc_df = withEventTimedf.orderBy(col(\"score_int\").asc())\n",
    "\n",
    "highest_scores = ordered_desc_df.limit(10)\n",
    "lowest_scores = ordered_asc_df.where(\"score_int is not null\").limit(10)\n",
    "\n",
    "corr_score1 = highest_scores.select(corr(\"num_comments_int\", \"score_int\").alias(\"Score_Comment_Corr\"))\n",
    "corr_score1 = corr_score1.withColumn(\"Group\", lit(\"High Score\")) \n",
    "corr_score2 = lowest_scores.select(corr(\"num_comments_int\", \"score_int\").alias(\"Score_Comment_Corr\"))\n",
    "corr_score2 = corr_score2.withColumn(\"Group\", lit(\"Low Score\"))\n",
    "\n",
    "resultsdf = corr_score1.union(corr_score2)\n",
    "resultsdf = resultsdf.withColumn(\"value\", col(\"Score_Comment_Corr\").cast(\"string\"))\n",
    "\n",
    "resultsdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
